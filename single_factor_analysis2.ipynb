{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "import datetime \n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "def runQuery(query,connexion):\n",
    "    cur = connexion.cursor()\n",
    "    cur.execute(query)\n",
    "    res = cur.fetchall()\n",
    "    return pd.DataFrame(list(res),columns = [e[0] for e in cur.description])\n",
    "\n",
    "class GoldenTimer:\n",
    "    def __init__(self, show=True):\n",
    "        self.start_time = time.time()\n",
    "        self.show = show\n",
    "\n",
    "    def time(self, print_str):\n",
    "        duration = time.time() - self.start_time\n",
    "        if self.show:\n",
    "            print(print_str, duration)\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "def missing_fun(data):\n",
    "    missing_df = data.dtypes.to_frame(\"type\").reset_index()\n",
    "    percent_missing = data.isnull().sum() * 100 / len(data)\n",
    "    nunique = data.nunique(dropna=False).values\n",
    "    missing_df[\"percent_missing\"] = percent_missing.values\n",
    "    missing_df[\"nunique\"] = nunique\n",
    "    missing_df = missing_df.loc[missing_df.percent_missing !=0]\n",
    "    missing_df = missing_df.sort_values(by=\"percent_missing\", ascending=False)\n",
    "    missing_df[\"min\"], missing_df[\"max\"] = np.nan, np.nan\n",
    "    missing_df.loc[missing_df[\"type\"]!=\"object\",\"min\"] = missing_df.loc[missing_df[\"type\"]!=\"object\",\"index\"].apply(lambda x:data[x].min())\n",
    "    missing_df.loc[missing_df[\"type\"]!=\"object\",\"max\"] = missing_df.loc[missing_df[\"type\"]!=\"object\",\"index\"].apply(lambda x:data[x].max())\n",
    "    missing_df[\"sample\"] = missing_df[\"index\"].apply(lambda x:data[x].value_counts(dropna=False).index.tolist())\n",
    "    return missing_df\n",
    "\n",
    "def plot_cm(y_true, y_pred, title):\n",
    "    figsize=(5,5)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    "    \n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  #tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    assert len(trn_series) == len(target)\n",
    "    #assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    \"\"\"\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    \"\"\"\n",
    "    return add_noise(ft_trn_series, noise_level) #, add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "def stat_fun(data_out,outlier_cols):\n",
    "    # Removing skewnewss and kurtosis using log transformation if it is above a threshold value (2)\n",
    "    data_stat = pd.DataFrame()\n",
    "    data_stat['numeric_column'] = outlier_cols\n",
    "    skew_before = []\n",
    "    skew_after = []\n",
    "\n",
    "    kurt_before = []\n",
    "    kurt_after = []\n",
    "\n",
    "    standard_deviation_before = []\n",
    "    standard_deviation_after = []\n",
    "\n",
    "    log_transform_needed = []\n",
    "\n",
    "    log_type = []\n",
    "\n",
    "    for i in outlier_cols:\n",
    "        skewval = data_out[i].skew()\n",
    "        skew_before.append(skewval)\n",
    "\n",
    "        kurtval = data_out[i].kurtosis()\n",
    "        kurt_before.append(kurtval)\n",
    "\n",
    "        sdval = data_out[i].std()\n",
    "        standard_deviation_before.append(sdval)\n",
    "\n",
    "        if (abs(skewval) > 2) & (abs(kurtval) > 2):\n",
    "            log_transform_needed.append('Yes')\n",
    "\n",
    "            if len(data_out[data_out[i] == 0])/len(data_out) <=0:\n",
    "                log_type.append('log')\n",
    "                skewvalnew = np.log(pd.DataFrame(data_out[data_out[i] > 0])[i]).skew()\n",
    "                skew_after.append(skewvalnew)\n",
    "\n",
    "                kurtvalnew = np.log(pd.DataFrame(data_out[data_out[i] > 0])[i]).kurtosis()\n",
    "                kurt_after.append(kurtvalnew)\n",
    "\n",
    "                sdvalnew = np.log(pd.DataFrame(data_out[data_out[i] > 0])[i]).std()\n",
    "                standard_deviation_after.append(sdvalnew)\n",
    "\n",
    "            else:\n",
    "                log_type.append('log1p')\n",
    "                skewvalnew = np.log1p(pd.DataFrame(data_out[data_out[i] >= 0])[i]).skew()\n",
    "                skew_after.append(skewvalnew)\n",
    "\n",
    "                kurtvalnew = np.log1p(pd.DataFrame(data_out[data_out[i] >= 0])[i]).kurtosis()\n",
    "                kurt_after.append(kurtvalnew)\n",
    "\n",
    "                sdvalnew = np.log1p(pd.DataFrame(data_out[data_out[i] >= 0])[i]).std()\n",
    "                standard_deviation_after.append(sdvalnew)\n",
    "\n",
    "        else:\n",
    "            log_type.append('NA')\n",
    "            log_transform_needed.append('No')\n",
    "\n",
    "            skew_after.append(skewval)\n",
    "            kurt_after.append(kurtval)\n",
    "            standard_deviation_after.append(sdval)\n",
    "\n",
    "    data_stat['skew_before'] = skew_before\n",
    "    data_stat['kurtosis_before'] = kurt_before\n",
    "    data_stat['standard_deviation_before'] = standard_deviation_before\n",
    "    data_stat['log_transform_needed'] = log_transform_needed\n",
    "    data_stat['log_type'] = log_type\n",
    "    data_stat['skew_after'] = skew_after\n",
    "    data_stat['kurtosis_after'] = kurt_after\n",
    "    data_stat['standard_deviation_after'] = standard_deviation_after\n",
    "    \n",
    "    return data_stat\n",
    "\n",
    "def outlier_fun(data_out,data_stat,outlier_cols):\n",
    "    outlier_df = pd.DataFrame(columns=[\"feature\", \"log\", \"count\", \"inc_count\", \"lower\",\"upper\"])\n",
    "    outlier_df[\"feature\"] = outlier_cols\n",
    "    card_list = []\n",
    "    cols_obs = []\n",
    "    out_count = 0\n",
    "    for colname in outlier_cols:\n",
    "        if data_stat.loc[data_stat[\"numeric_column\"] == colname, \"log_transform_needed\"].values==\"Yes\":\n",
    "            data_temp = data_out[[\"cc_nbr\",colname]]\n",
    "            if data_stat.loc[data_stat[\"numeric_column\"] == colname, \"log_type\"].values==\"log\":\n",
    "                data_temp[colname + \"_log\"] = np.log(data_temp[colname]) \n",
    "                temp_card = data_temp.loc[(np.abs(sp.stats.zscore(data_temp[colname+\"_log\"])) > 3),\"cc_nbr\"].values.tolist()\n",
    "                lower = data_temp.loc[(sp.stats.zscore(data_temp[colname+\"_log\"]) < -3),colname].max()\n",
    "                upper = data_temp.loc[(sp.stats.zscore(data_temp[colname+\"_log\"]) > 3),colname].min()\n",
    "                log = \"log\"\n",
    "                card_list.extend(temp_card)\n",
    "                #print(f\"Log transformation made to {colname} column ; There are {len(temp_card)} outliers\")\n",
    "            if data_stat.loc[data_stat[\"numeric_column\"] == colname,\"log_type\"].values==\"log1p\":\n",
    "                data_temp[colname + \"_log1p\"] = np.log1p(data_temp[colname]) \n",
    "                temp_card = data_temp.loc[(np.abs(sp.stats.zscore(data_temp[colname+\"_log1p\"])) > 3),\"cc_nbr\"].values.tolist()\n",
    "                lower = data_temp.loc[(sp.stats.zscore(data_temp[colname+\"_log1p\"]) < -3),colname].max()\n",
    "                upper = data_temp.loc[(sp.stats.zscore(data_temp[colname+\"_log1p\"]) > 3),colname].min()\n",
    "                log = \"log1p\"\n",
    "                card_list.extend(temp_card)\n",
    "                #print(f\"Log1p transformation made to {colname} column ; There are {len(temp_card)} outliers\")\n",
    "        else:\n",
    "            data_temp = data_out[[\"cc_nbr\",colname]]\n",
    "            temp_card = data_temp.loc[(np.abs(sp.stats.zscore(data_temp[colname])) > 3),\"cc_nbr\"].values.tolist()\n",
    "            lower = data_temp.loc[(sp.stats.zscore(data_temp[colname]) < -3),colname].max()\n",
    "            upper = data_temp.loc[(sp.stats.zscore(data_temp[colname]) > 3),colname].min()\n",
    "            log = \"no\"\n",
    "            card_list.extend(temp_card)\n",
    "            #print(f\"No Log transformation made to {colname} column ; There are {len(temp_card)} outliers\")\n",
    "        if len(temp_card)>5000:\n",
    "            cols_obs.append(colname)\n",
    "        inc_count = len(set(card_list)) - out_count\n",
    "        outlier_df.loc[outlier_df[\"feature\"]==colname,[\"log\",\"count\", \"inc_count\",\"lower\",\"upper\"]] = [log,len(temp_card), inc_count, lower, upper]\n",
    "        out_count = len(set(card_list))\n",
    "    print(f\"There are {len(set(card_list))} outlier Credit Cards\")\n",
    "    return outlier_df, card_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_file(filename='single_factor_analysis.html'):\n",
    "    with open(filename, mode = \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_value(df, target='M6_F', org_feature='ResearchVar', rank_feature='RankVar'):\n",
    "    try:\n",
    "        nnull_data = df[~df[org_feature].isnull()].copy()\n",
    "        c_org = roc_auc_score(nnull_data[target],nnull_data[org_feature])\n",
    "        c_ranked = roc_auc_score(nnull_data[target],nnull_data[rank_feature])\n",
    "        if c_org <= 0.5:\n",
    "            c_org = 1-c_org\n",
    "        if c_ranked <= 0.5:\n",
    "            c_ranked = 1-c_ranked\n",
    "        return c_org, c_ranked\n",
    "    except:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_value(df, target='M6_F', org_feature='ResearchVar', rank_feature ='RankVar', rank_null_tr=-1, id_column=\"TALEP_NO\"):\n",
    "    try:\n",
    "        tr_data = df.copy()\n",
    "        tr_data[rank_feature] = tr_data[rank_feature].fillna(rank_null_tr)\n",
    "    \n",
    "        pv = tr_data.pivot_table(values=id_column, index=rank_feature, columns=target,aggfunc='count')\n",
    "        pv.columns=['good','bad']\n",
    "        num_good = pv.sum(axis=0)['good']\n",
    "        num_bad = pv.sum(axis=0)['bad']\n",
    "        pv['pc_good'] = pv['good']/num_good\n",
    "        pv['pc_bad'] = pv['bad']/num_bad\n",
    "        pv['woe']=np.log(pv['pc_good']/pv['pc_bad'])\n",
    "        pv['iv'] = (pv['pc_good'] - pv['pc_bad']) * pv['woe']\n",
    "        iv = pv['iv'].sum()\n",
    "        return iv\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('./single_factor_analysis/sfa_code.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data & Feature Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nas(main, dtypes_df):\n",
    "    for col in dtypes_df.feature:\n",
    "        if col in cat_features:\n",
    "            main[col].fillna(\"Missing\",inplace=True)\n",
    "        else:\n",
    "            main[col] = main[col].apply(lambda x: x if (x!=None) and (x!=np.nan) and (np.isnan(x)!=True) else main[col].mean())\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(\"datathon_yedek_parca_catalog.csv\",sep=\"|\")\n",
    "orders = pd.read_csv(\"datathon_yedek_parca_orders.csv\",sep=\"|\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\",sep=\"|\")\n",
    "df = orders.merge(catalog,\"left\",\"part_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"stock_card_create_date\"] = pd.to_datetime(df[\"stock_card_create_date\"])\n",
    "df[\"period\"] = pd.to_datetime(df[\"date\"]).dt.to_period('m')\n",
    "\n",
    "df[\"part_season\"] = df[\"part_season\"].map({'Yaz Mevsimsel':1,'Kış Mevsimsel':0}).fillna(-1)\n",
    "df[\"min_date\"] = df.groupby(\"part_id\")[\"date\"].transform(\"min\")\n",
    "df[\"stock_card_create_date\"] = df[[\"min_date\",\"stock_card_create_date\"]].min(axis = 1)\n",
    "del df[\"min_date\"]\n",
    "\n",
    "# period = df[[\"period\"]].drop_duplicates().sort_values(\"period\").reset_index(drop=True)\n",
    "period = pd.concat([df[[\"period\"]].drop_duplicates().sort_values(\"period\"),submission[[\"period\"]].drop_duplicates()]).reset_index(drop=True)\n",
    "period[\"period\"] = period[\"period\"].astype(\"period[M]\")\n",
    "\n",
    "ids = df[['part_id', 'firm_id', 'part_definition_id','part_product_class_id', \n",
    "            'common_part_catalog_id','preferred_supplier_id', 'part_family_id',\n",
    "            'stock_card_create_date','part_season']].drop_duplicates()\n",
    "\n",
    "period['key'] = 0\n",
    "ids['key'] = 0\n",
    "join = period.merge(ids, how='outer')\n",
    "\n",
    "del join[\"key\"], period, ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby(['part_id', 'firm_id', 'part_definition_id','part_product_class_id', \n",
    "                   'common_part_catalog_id','preferred_supplier_id', 'part_family_id',\n",
    "                   'stock_card_create_date','part_season', 'period'])[\"order_quantity\"].agg([\"sum\"])\\\n",
    "                    .reset_index().rename({\"sum\":'order_quantity'},axis=1)\n",
    "\n",
    "data[\"min_period\"] = data.groupby([\"part_id\",\"firm_id\"])[\"period\"].transform(\"min\")\n",
    "print(data.shape)\n",
    "\n",
    "data = join.merge(data,\"left\",['part_id', 'firm_id', 'part_definition_id','part_product_class_id', \n",
    "                               'common_part_catalog_id','preferred_supplier_id', 'part_family_id',\n",
    "                               'stock_card_create_date','part_season', 'period'])\n",
    "\n",
    "data[\"min_period\"] = data.groupby([\"part_id\",\"firm_id\"])[\"min_period\"].apply(lambda x: x.fillna(method=\"bfill\").fillna(method=\"ffill\"))\n",
    "data[\"order_quantity\"] = data[\"order_quantity\"].fillna(0)\n",
    "print(data.shape)\n",
    "\n",
    "data = data[data[\"period\"]>=data[\"min_period\"]]\n",
    "del data[\"min_period\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"time\"] = data[\"period\"].rank(method = \"dense\")\n",
    "\n",
    "df = df.sort_values([\"part_id\",\"date\"]).reset_index(drop=True)\n",
    "df[\"time\"] = df[\"period\"].rank(method = \"dense\")\n",
    "df[\"time_diff\"] = df.groupby([\"part_id\"])[\"time\"].diff()\n",
    "time_diff_mean = df.groupby([\"part_id\"])[\"time_diff\"].mean().to_dict()\n",
    "time_diff_std = df.groupby([\"part_id\"])[\"time_diff\"].std().to_dict()\n",
    "time_diff_min = df.groupby([\"part_id\"])[\"time_diff\"].min().to_dict()\n",
    "time_diff_max = df.groupby([\"part_id\"])[\"time_diff\"].max().to_dict()\n",
    "\n",
    "df = df.sort_values([\"part_id\",\"firm_id\",\"date\"]).reset_index(drop=True)\n",
    "df[\"time_diff_firm\"] = df.groupby([\"part_id\",\"firm_id\"])[\"time\"].diff()\n",
    "\n",
    "time_diff_firm_mean = df.groupby([\"part_id\"])[\"time_diff_firm\"].mean().to_dict()\n",
    "time_diff_firm_std = df.groupby([\"part_id\"])[\"time_diff_firm\"].std().to_dict()\n",
    "time_diff_firm_min = df.groupby([\"part_id\"])[\"time_diff_firm\"].min().to_dict()\n",
    "time_diff_firm_max = df.groupby([\"part_id\"])[\"time_diff_firm\"].max().to_dict()\n",
    "\n",
    "df[\"order_quantity_first\"] = df.groupby([\"part_id\",\"firm_id\"])[\"order_quantity\"].transform(\"first\")\n",
    "order_quantity_first_mean = df.groupby([\"part_id\"])[\"order_quantity_first\"].mean().to_dict()\n",
    "order_quantity_first_std = df.groupby([\"part_id\"])[\"order_quantity_first\"].std().to_dict()\n",
    "\n",
    "df[\"order_quantity_last\"] = df.groupby([\"part_id\",\"firm_id\"])[\"order_quantity\"].transform(\"last\")\n",
    "order_quantity_last_mean = df.groupby([\"part_id\"])[\"order_quantity_last\"].mean().to_dict()\n",
    "order_quantity_last_std = df.groupby([\"part_id\"])[\"order_quantity_last\"].std().to_dict()\n",
    "\n",
    "df[\"order_quantity_min\"] = df.groupby([\"part_id\",\"firm_id\"])[\"order_quantity\"].transform(\"min\")\n",
    "order_quantity_min_mean = df.groupby([\"part_id\"])[\"order_quantity_min\"].mean().to_dict()\n",
    "order_quantity_min_std = df.groupby([\"part_id\"])[\"order_quantity_min\"].std().to_dict()\n",
    "\n",
    "df[\"order_quantity_max\"] = df.groupby([\"part_id\",\"firm_id\"])[\"order_quantity\"].transform(\"max\")\n",
    "order_quantity_max_mean = df.groupby([\"part_id\"])[\"order_quantity_max\"].mean().to_dict()\n",
    "order_quantity_max_std = df.groupby([\"part_id\"])[\"order_quantity_max\"].std().to_dict()\n",
    "\n",
    "data = data.sort_values([\"part_id\",\"firm_id\",\"period\"]).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"last_time\"] = data.loc[data[\"order_quantity\"]>0,\"time\"]\n",
    "data[\"last_time\"] = data.groupby([\"part_id\",\"firm_id\"])[\"last_time\"].apply(lambda x: x.fillna(method=\"ffill\"))\n",
    "data[\"last_time\"] = data[\"time\"] - data[\"last_time\"]\n",
    "\n",
    "last_time_mean = data.groupby([\"part_id\",\"period\"])[\"last_time\"].mean().reset_index().rename({\"last_time\":\"last_time_mean\"},axis=1)\n",
    "last_time_std = data.groupby([\"part_id\",\"period\"])[\"last_time\"].std().reset_index().rename({\"last_time\":\"last_time_std\"},axis=1)\n",
    "\n",
    "train = data.groupby(['part_id', 'part_definition_id','part_product_class_id', 'common_part_catalog_id',\n",
    "                      'preferred_supplier_id', 'part_family_id','stock_card_create_date','part_season', 'period'])[\"order_quantity\"].agg([\"sum\"])\\\n",
    "                     .reset_index().rename({\"sum\":'order_quantity'},axis=1).sort_values([\"part_id\",\"period\"])\n",
    "train[\"time\"] = train[\"period\"].rank(method = \"dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm([1,2,3,6]):\n",
    "    train[f\"order_quantity_lag{i}\"] = train.groupby(\"part_id\")[\"order_quantity\"].shift(i)\n",
    "    \n",
    "    for j in [6,12]:\n",
    "        train[f'order_quantity_lag{i}_w{j}_mean'] = train.groupby(['part_id'])[\"order_quantity\"].transform(lambda x: x.shift(i).rolling(j).mean())\n",
    "        train[f'order_quantity_lag{i}_w{j}_std'] = train.groupby(['part_id'])[\"order_quantity\"].transform(lambda x: x.shift(i).rolling(j).std())\n",
    "        \n",
    "train[\"order_quantity_lag1_w12_w6_ratio\"] = train[\"order_quantity_lag1_w12_mean\"]/train[\"order_quantity_lag1_w6_mean\"]\n",
    "train[\"order_quantity_lag2_w12_w6_ratio\"] = train[\"order_quantity_lag2_w12_mean\"]/train[\"order_quantity_lag2_w6_mean\"]\n",
    "train[\"order_quantity_lag3_w12_w6_ratio\"] = train[\"order_quantity_lag3_w12_mean\"]/train[\"order_quantity_lag3_w6_mean\"]\n",
    "train[\"order_quantity_lag6_w12_w6_ratio\"] = train[\"order_quantity_lag6_w12_mean\"]/train[\"order_quantity_lag6_w6_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(['part_product_class_id', 'part_family_id']):\n",
    "    train[f\"order_{col}\"] = train.groupby([col,\"period\"])[\"order_quantity\"].transform(\"sum\")\n",
    "    for i in [1,2,3]:\n",
    "        train[f\"order_{col}_lag{i}\"] = train.groupby(col)[f\"order_{col}\"].shift(i)\n",
    "        \n",
    "    del train[f\"order_{col}\"]\n",
    "\n",
    "train = train.sort_values([\"part_id\",\"period\"])\n",
    "train[\"order_normalized\"] = train.groupby([\"part_id\"])[\"order_quantity\"].transform(\"cumsum\")/(train[\"time\"] - train.groupby([\"part_id\"])[\"time\"].transform(\"min\") + 1)\n",
    "for i in tqdm([1,2,3]):\n",
    "    train[f\"order_normalized_lag{i}\"] = train.groupby(\"part_id\")[\"order_normalized\"].shift(i)\n",
    "    \n",
    "del train[\"order_normalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"preferred_supplier_id\",\"common_part_catalog_id\"]:\n",
    "    train[f\"{col}_freq\"] = train.groupby([col])[\"time\"].transform(\"count\")/len(train)\n",
    "\n",
    "parts = train.groupby([\"part_id\",\"time\"])[\"order_quantity\"].agg([\"sum\"]).reset_index().pivot_table(\"sum\",[\"part_id\"],\"time\").fillna(0).reset_index()\n",
    "time_cols = parts.columns[1:].tolist()\n",
    "parts[\"min\"] = parts[time_cols].min(axis=1)\n",
    "parts[\"max\"] = parts[time_cols].max(axis=1)\n",
    "parts[\"median\"] = parts[time_cols].median(axis=1)\n",
    "\n",
    "X = parts.iloc[:,1:].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n",
    "dist, ind = tree.query(X, k=4)                # doctest: +SKIP\n",
    "print(ind)  # indices of 3 closest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts[\"neighbor1\"] = parts[\"neighbor2\"] = parts[\"neighbor3\"] = 0\n",
    "for i in range(5000):\n",
    "    parts[\"neighbor1\"].iloc[i] = parts[\"part_id\"].iloc[ind[i][1]]\n",
    "    parts[\"neighbor2\"].iloc[i] = parts[\"part_id\"].iloc[ind[i][2]]\n",
    "    parts[\"neighbor3\"].iloc[i] = parts[\"part_id\"].iloc[ind[i][3]]   \n",
    "    \n",
    "for col in [\"neighbor1\",\"neighbor2\",\"neighbor3\"]:\n",
    "    if col in train.columns: del train[col]\n",
    "train = train.merge(parts[[\"part_id\",\"neighbor1\",\"neighbor2\",\"neighbor3\"]],\"left\",\"part_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"time_diff_mean\"] = train[\"part_id\"].map(time_diff_mean)\n",
    "train[\"time_diff_std\"] = train[\"part_id\"].map(time_diff_std)\n",
    "train[\"time_diff_firm_mean\"] = train[\"part_id\"].map(time_diff_firm_mean)\n",
    "train[\"time_diff_firm_std\"] = train[\"part_id\"].map(time_diff_firm_std)\n",
    "\n",
    "firm_count_mean = df.groupby([\"part_id\",\"firm_id\"])[\"time\"].count().reset_index().groupby(\"part_id\")[\"time\"].mean().to_dict()\n",
    "train[\"firm_count_mean\"] = train[\"part_id\"].map(firm_count_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_nunique = df.groupby([\"part_id\",\"period\"])[\"firm_id\"].nunique().reset_index().rename({\"firm_id\":\"firm_nunique\"},axis = 1)\n",
    "train = train.merge(firm_nunique,\"left\",[\"part_id\",\"period\"])\n",
    "for col in [\"firm_nunique\"]:\n",
    "    for i in tqdm([1,2,3]):\n",
    "        train[f\"{col}_lag{i}\"] = train.groupby(\"part_id\")[col].shift(i)\n",
    "        \n",
    "    del train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"order_quantity_first_mean\"] = train[\"part_id\"].map(order_quantity_first_mean)\n",
    "train[\"order_quantity_first_std\"] = train[\"part_id\"].map(order_quantity_first_std)\n",
    "\n",
    "train[\"order_quantity_last_mean\"] = train[\"part_id\"].map(order_quantity_last_mean)\n",
    "train[\"order_quantity_last_std\"] = train[\"part_id\"].map(order_quantity_last_std)\n",
    "\n",
    "train[\"order_quantity_first_last\"] = train[\"order_quantity_first_mean\"] - train[\"order_quantity_last_mean\"]\n",
    "\n",
    "train[\"order_quantity_min_mean\"] = train[\"part_id\"].map(order_quantity_min_mean)\n",
    "train[\"order_quantity_min_std\"] = train[\"part_id\"].map(order_quantity_min_std)\n",
    "\n",
    "train[\"order_quantity_max_mean\"] = train[\"part_id\"].map(order_quantity_max_mean)\n",
    "train[\"order_quantity_max_std\"] = train[\"part_id\"].map(order_quantity_max_std)\n",
    "\n",
    "train[\"order_quantity_min_max\"] = train[\"order_quantity_min_mean\"] / train[\"order_quantity_max_mean\"]\n",
    "del train[\"order_quantity_min_mean\"], train[\"order_quantity_min_std\"]\n",
    "\n",
    "train = train.merge(last_time_mean,\"left\",[\"part_id\",\"period\"]).merge(last_time_std,\"left\",[\"part_id\",\"period\"])\n",
    "for col in [\"last_time_mean\",\"last_time_std\"]:\n",
    "    for i in tqdm([1,2,3]):\n",
    "        train[f\"{col}_lag{i}\"] = train.groupby(\"part_id\")[col].shift(i)\n",
    "        \n",
    "    del train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFA - Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main = pd.read_csv('./data/model_data_jan.csv')\n",
    "# main.reset_index(inplace=True)\n",
    "# main.rename(columns={\"index\":\"id\",\"Churn.30\":\"TARGET\"},inplace=True)\n",
    "# main = fill_nas(main,sfa_owner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_file(\"single_factor_analysis_jan.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['time_diff_std','time_diff_mean','order_normalized_lag3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.time.isin([39,38,37])==False]\n",
    "train.reset_index(inplace=True)\n",
    "train.rename({'index':'id'},axis=1,inplace=True)\n",
    "train.rename({'order_quantity':'TARGET'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['order_normalized_lag3'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== time_diff_std ===\n",
      "Number of not-null observations: 167059\n",
      "OK for time_diff_std\n",
      "=== time_diff_mean ===\n",
      "Number of not-null observations: 167059\n",
      "OK for time_diff_mean\n",
      "=== order_normalized_lag3 ===\n",
      "Number of not-null observations: 167059\n",
      "OK for order_normalized_lag3\n",
      "Finished Continuous SFA\n"
     ]
    }
   ],
   "source": [
    "c_values = []\n",
    "graph_stats_dfs_cont = []\n",
    "# with open('single_factor_analysis_jan.html', mode = \"a\", encoding=\"utf-8\") as file:\n",
    "#     file.write(\"<b>Continuous Variables Analysis\")\n",
    "    \n",
    "for feat in cont_features:\n",
    "    print(\"=\"*3+\" \"+feat+\" \"+\"=\"*3)\n",
    "    graph_stats, stats_df, minimum, maximum = sfa_cont(main, feat, 'TARGET', 6,id_column=\"id\")\n",
    "\n",
    "    exec(open('./single_factor_analysis/draw_table.py').read())\n",
    "    pio.write_image(fig0, './single_factor_analysis/images/table/'+feat+'_table.png')\n",
    "\n",
    "    exec(open('./single_factor_analysis/draw_graph.py').read())\n",
    "    pio.write_image(fig, './single_factor_analysis/images/graph/'+feat+'_graph.png')\n",
    "\n",
    "    c_values.append([feat,stats_df.iloc[0]['org c-value']])\n",
    "    graph_stats_dfs_cont.append([feat,graph_stats])\n",
    "\n",
    "    # file.write(pio.to_html(fig, default_width = '50%', default_height = '50%'))\n",
    "    # file.write(pio.to_html(fig0))\n",
    "    print ('OK for '+feat)\n",
    "\n",
    "c_values_df = pd.DataFrame(c_values, columns=['feature','c_value'])\n",
    "# c_values_df.sort_values(by='c_value',ascending=False).to_excel('./cont_feature_ranking.xlsx')\n",
    "c_values_df.sort_values(by='c_value',ascending=False,inplace=True)\n",
    "# file.write(\"<b>C-Values Table</b>\")\n",
    "# file.write(c_values_df.to_html())\n",
    "print('Finished Continuous SFA')\n",
    "graph_stats_dfs_cont = pd.DataFrame(graph_stats_dfs_cont,columns=['feature','stats_df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFA - Categoric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_values = []\n",
    "graph_stats_dfs_cat=[]\n",
    "# with open('zipcode_sfa.html', mode = \"a\", encoding=\"utf-8\") as file:\n",
    "#     file.write(\"<b>Continuous Variables Analysis\")\n",
    "\n",
    "for feat in cat_features:\n",
    "#     if feat.find(\"zip_code\")!=-1 or feat.find(\"locality\")!=-1:\n",
    "        # print(\"=\"*3+\" \"+feat+\" \"+\"=\"*3)\n",
    "    graph_stats, stats_df = sfa_cat(main, feat, 'TARGET',id_column=\"id\")\n",
    "\n",
    "    exec(open('./single_factor_analysis/draw_table_cat.py').read())\n",
    "    pio.write_image(fig0, './single_factor_analysis/images/table/'+feat+'_table.png')\n",
    "\n",
    "    exec(open('./single_factor_analysis/draw_graph_cat.py').read())\n",
    "    pio.write_image(fig, './single_factor_analysis/images/graph/'+feat+'_graph.png')\n",
    "    information_values.append([feat,stats_df.iloc[0]['iv']])\n",
    "    graph_stats_dfs_cat.append([feat,graph_stats])\n",
    "\n",
    "#             file.write(pio.to_html(fig, default_width = '50%', default_height = '50%'))\n",
    "#             file.write(pio.to_html(fig_cat))\n",
    "    print ('OK for '+feat)\n",
    "        \n",
    "information_values_df = pd.DataFrame(information_values, columns=['feature','iv'])\n",
    "information_values_df.sort_values(by='iv',ascending=False,inplace=True)\n",
    "    # information_values_df.sort_values(by='iv',ascending=False).to_excel('./cont_feature_ranking.xlsx')\n",
    "#     file.write(\"<b>IV Table</b>\")\n",
    "#     file.write(information_values_df.to_html())\n",
    "print('Finished Categorical SFA')\n",
    "graph_stats_dfs_cat = pd.DataFrame(graph_stats_dfs_cat,columns=['feature','stats_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_familia(x):\n",
    "    iy = x.find(\"transformed\")\n",
    "    ix = x.find(\"clustered\")\n",
    "    if x.find(\"transformed\")!=-1 or x.find(\"clustered\")!=-1:\n",
    "        return x[:(max(ix,iy)-1)]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_values_df[\"familia\"] = information_values_df.feature.apply(find_familia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_values_df.to_excel(\"ivs_jan_2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Clustering (Categorical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_scheme(graph_stats_dfs_cat, steps=2, max_clusters= 10):\n",
    "    clustering_scheme = {}\n",
    "    for i in graph_stats_dfs_cat.index:\n",
    "        tmp = graph_stats_dfs_cat.loc[i].stats_df\n",
    "        n=tmp.shape[0]\n",
    "        if n>=max_clusters:\n",
    "            n=max_clusters\n",
    "        if n>2:\n",
    "            clustering_scheme[tmp.index.name] = list(range(2,n,steps))\n",
    "    return clustering_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans(graph_stats_dfs_cat, feature, n_clusters=2):\n",
    "    try:\n",
    "        tmp = graph_stats_dfs_cat[graph_stats_dfs_cat.feature==feature].iloc[0].stats_df\n",
    "        X = np.array([[e,f] for e,f in zip([1]*tmp.shape[0],tmp.target_ratio.values)])\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "        tmp[\"category\"]=kmeans.labels_\n",
    "        cluster_map = {}\n",
    "        for cat in tmp.category.unique():\n",
    "            cluster_map[cat]=tmp[tmp['category']==cat].index.tolist()\n",
    "        print(feature+\" splitted to \"+str(n_clusters)+\" categories.\")\n",
    "        return cluster_map\n",
    "    except:\n",
    "        print(feature+\" can't be splitted to \"+str(n_clusters)+\" categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster(x,cluster_map):\n",
    "    for key,value in cluster_map.items():\n",
    "        if(x in value):\n",
    "            return key\n",
    "    print(\"cant find \"+str(x)+\" in cluster_map\")\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clusters_to_data(main,graph_stats_dfs_cat,feature,n_clusters=5,suffix=\"_clustered\"):\n",
    "    cluster_map = apply_kmeans(graph_stats_dfs_cat,feature, n_clusters)\n",
    "    main[feature+suffix+\"_\"+str(len(cluster_map.keys()))] = main[feature].apply(find_cluster,args=(cluster_map,))\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_scheme = get_clustering_scheme(graph_stats_dfs_cat,steps=3,max_clusters=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in clustering_scheme.keys():\n",
    "    for i in clustering_scheme[feature]:\n",
    "        print(feature + \" - \"+ str(i))\n",
    "        main = apply_clusters_to_data(main, graph_stats_dfs_cat, feature, n_clusters=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Threshold (Continuous Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"total_segundos_espera_transformed\"] = (main[\"total_segundos_espera\"]>25)*1\n",
    "main[\"total_segundos_conversacion_transformed\"] = (main[\"total_segundos_conversacion\"]>75)*1\n",
    "main[\"ranking_transformed\"] = (main[\"ranking\"]==1)*1\n",
    "main[\"tariff_ds_fibra_quota_transformed\"] = (main[\"tariff_ds_fibra_quota\"]==300)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.to_csv('./data/model_data_jan.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
